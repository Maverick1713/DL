OR GATE:


def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    w1, w2, b = 0, 0, 0
    desired_outputs = [0, 0, 0, 1]
    learning_rate = 0.1
    epochs = 100

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            A, B = inputs[i]
            target_output = desired_outputs[i]
            output = activate(w1 * A + w2 * B + b)
            error = target_output - output
            w1 += learning_rate * error * A
            w2 += learning_rate * error * B
            b += learning_rate * error
            total_error += abs(error)
        if total_error == 0:
            break

    if total_error == 0:
        return w1, w2, b

inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
w1, w2, b = perceptron(inputs)

print("Weights:", w1, w2)
print("Bias:", b)
for i in range(len(inputs)):
    A, B = inputs[i]
    output = activate(w1 * A + w2 * B + b)
    print("Input:", A, B, "Output:", output)



AND GATE :

def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    w1, w2, b = 0, 0, 0
    desired_outputs = [0, 1, 1, 1]
    learning_rate = 0.1
    epochs = 100

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            A, B = inputs[i]
            target_output = desired_outputs[i]
            output = activate(w1 * A + w2 * B + b)
            error = target_output - output
            w1 += learning_rate * error * A
            w2 += learning_rate * error * B
            b += learning_rate * error
            total_error += abs(error)
        if total_error == 0:
            break

    if total_error == 0:
        return w1, w2, b

inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
w1, w2, b = perceptron(inputs)

print("Weights:", w1, w2)
print("Bias:", b)
for i in range(len(inputs)):
    A, B = inputs[i]
    output = activate(w1 * A + w2 * B + b)
    print("Input:", A, B, "Output:", output)


NOR GATE:  


def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    w1, w2, b = 0, 0, 0
    desired_outputs = [1, 0, 0, 0]
    learning_rate = 0.1
    epochs = 100

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            A, B = inputs[i]
            target_output = desired_outputs[i]
            output = activate(w1 * A + w2 * B + b)
            error = target_output - output
            w1 += learning_rate * error * A
            w2 += learning_rate * error * B
            b += learning_rate * error
            total_error += abs(error)
        if total_error == 0:
            break

    if total_error == 0:
        return w1, w2, b

inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
w1, w2, b = perceptron(inputs)

print("Weights:", w1, w2)
print("Bias:", b)
for i in range(len(inputs)):
    A, B = inputs[i]
    output = activate(w1 * A + w2 * B + b)
    print("Input:", A, B, "Output:", output)


NAND GATE :::


def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    w1, w2, b = 0, 0, 0
    desired_outputs = [1, 1, 1,0]
    learning_rate = 0.1
    epochs = 100

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            A, B = inputs[i]
            target_output = desired_outputs[i]
            output = activate(w1 * A + w2 * B + b)
            error = target_output - output
            w1 += learning_rate * error * A
            w2 += learning_rate * error * B
            b += learning_rate * error
            total_error += abs(error)
        if total_error == 0:
            break

    if total_error == 0:
        return w1, w2, b

inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
w1, w2, b = perceptron(inputs)

print("Weights:", w1, w2)
print("Bias:", b)
for i in range(len(inputs)):
    A, B = inputs[i]
    output = activate(w1 * A + w2 * B + b)
    print("Input:", A, B, "Output:", output)


NOT GATE::::


def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    w1, b = 0, -1
    desired_outputs = [1, 0]
    learning_rate = 0.1
    epochs = 100

    for epoch in range(epochs):
        total_error = 0
        for i in range(len(inputs)):
            A = inputs[i]
            target_output = desired_outputs[i]
            output = activate(w1 * A + b)
            error = target_output - output
            w1 += learning_rate * error * A
            b += learning_rate * error
            total_error += abs(error)
        if total_error == 0:
            break

    if total_error == 0:
        return w1, b

inputs = [0, 1]
w1, b = perceptron(inputs)

print("NOT Gate Output:")
print("Weight:", w1)
print("Bias:", b)
for i in range(len(inputs)):
    A = inputs[i]
    output = activate(w1 * A + b)
    print("Input:", A, "Output:", output)



XOR GATE:::::
import numpy as np

def unitStep(v):
  return 1 if v>=0 else 0

def perceptronModel(x, w, b):
  v = np.dot(w, x) + b
  y = unitStep(v)
  return y

def NOT_logicFunction(x):
  wNOT = -1
  bNOT = 0.5
  return perceptronModel(x, wNOT, bNOT)

def AND_logicFunction(x):
  w = np.array([1, 1])
  bAND = -1.5
  return perceptronModel(x, w, bAND)

def OR_logicFunction(x):
  w = np.array([1, 1])
  bOR = -0.5
  return perceptronModel(x, w, bOR)

def XOR_logicFunction(x):
  y1 = AND_logicFunction(x)
  y2 = OR_logicFunction(x)
  y3 = NOT_logicFunction(y1)
  final_x = np.array([y2, y3])
  finalOutput = AND_logicFunction(final_x)
  return finalOutput

test1 = np.array([0, 1])
test2 = np.array([1, 1])
test3 = np.array([0, 0])
test4 = np.array([1, 0])

print("XOR({}, {}) = {}".format(0, 1, XOR_logicFunction(test1)))
print("XOR({}, {}) = {}".format(1, 1, XOR_logicFunction(test2)))
print("XOR({}, {}) = {}".format(0, 0, XOR_logicFunction(test3)))
print("XOR({}, {}) = {}".format(1, 0, XOR_logicFunction(test4)))
